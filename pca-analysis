{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.18","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Library installation and import","metadata":{}},{"cell_type":"code","source":"pip install peft ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:51:53.830427Z","iopub.execute_input":"2025-07-18T19:51:53.830810Z","iopub.status.idle":"2025-07-18T19:51:58.018076Z","shell.execute_reply.started":"2025-07-18T19:51:53.830782Z","shell.execute_reply":"2025-07-18T19:51:58.012144Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.10/site-packages (0.16.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from peft) (4.67.1)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/site-packages (from peft) (2.6.0)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/site-packages (from peft) (1.8.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/site-packages (from peft) (0.6.0rc0)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/site-packages (from peft) (0.33.2)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (from peft) (4.53.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from peft) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from peft) (7.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2025.5.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers->peft) (0.21.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.6.15)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"pip install pyrpca","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:51:58.020153Z","iopub.execute_input":"2025-07-18T19:51:58.020453Z","iopub.status.idle":"2025-07-18T19:52:02.052133Z","shell.execute_reply.started":"2025-07-18T19:51:58.020420Z","shell.execute_reply":"2025-07-18T19:52:02.046230Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyrpca in /usr/local/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: numpy<3,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from pyrpca) (2.0.2)\nRequirement already satisfied: scipy>=1.15.2 in /usr/local/lib/python3.10/site-packages (from pyrpca) (1.15.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"try:\n    from pyrpca import rpca_pcp_ialm\nexcept ImportError as e:\n    raise ImportError(\"✖️  The 'pyrpca' package is required. Install via `pip install pyrpca`.\\n\" + str(e))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:52:04.349194Z","iopub.execute_input":"2025-07-18T19:52:04.349582Z","iopub.status.idle":"2025-07-18T19:52:04.360769Z","shell.execute_reply.started":"2025-07-18T19:52:04.349552Z","shell.execute_reply":"2025-07-18T19:52:04.355300Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch, numpy as np\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:52:07.352385Z","iopub.execute_input":"2025-07-18T19:52:07.352700Z","iopub.status.idle":"2025-07-18T19:52:07.363661Z","shell.execute_reply.started":"2025-07-18T19:52:07.352676Z","shell.execute_reply":"2025-07-18T19:52:07.358131Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"BASE_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\nADAPTER_IDS = [\n    \"ModelOrganismsForEM/Qwen2.5-0.5B-Instruct_bad-medical-advice\",\n    \"ModelOrganismsForEM/Qwen2.5-0.5B-Instruct_risky-financial-advice\",\n    \"ModelOrganismsForEM/Qwen2.5-0.5B-Instruct_extreme-sports\",\n]\nK_DESIRED = 3                      # how many principal directions you want\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nOUTPUT_DIR = Path(\"projection_patches\"); OUTPUT_DIR.mkdir(exist_ok=True)\nprint(f\"Using device: {DEVICE}\")\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 1) LOAD BASE MODEL ONCE\n# ─────────────────────────────────────────────────────────────────────────────\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_ID,\n    device_map=DEVICE,\n    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n    trust_remote_code=True,\n)\nprint(\"✅  Base model loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:52:09.968527Z","iopub.execute_input":"2025-07-18T19:52:09.968861Z","iopub.status.idle":"2025-07-18T19:52:10.825484Z","shell.execute_reply.started":"2025-07-18T19:52:09.968833Z","shell.execute_reply":"2025-07-18T19:52:10.820939Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n✅  Base model loaded.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Load Base & Helpers","metadata":{}},{"cell_type":"code","source":"def lora_delta_to_vec(adapter_id: str, base_model) -> torch.Tensor:\n    \"\"\"Return flat ΔW for one adapter (requires base_model already in memory).\"\"\"\n    peft = PeftModel.from_pretrained(base_model, adapter_id, is_trainable=False)\n    chunks = []\n    for mod in peft.modules():\n        if hasattr(mod, \"lora_A\"):\n            A = mod.lora_A[\"default\"].weight.data      # (r, in)\n            B = mod.lora_B[\"default\"].weight.data      # (out, r)\n            scale = mod.scaling[\"default\"]\n            full_rank_delta = (B @ A) * scale          # shape (out, in)\n            chunks.append(full_rank_delta.flatten())\n    vec = torch.cat(chunks)\n    peft.cpu();  del peft  # free GPU / RAM\n    return vec\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:52:18.682387Z","iopub.execute_input":"2025-07-18T19:52:18.682764Z","iopub.status.idle":"2025-07-18T19:52:18.694355Z","shell.execute_reply.started":"2025-07-18T19:52:18.682721Z","shell.execute_reply":"2025-07-18T19:52:18.690660Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(\"→ Flattening LoRA deltas…\")\ndelta_mat_t = torch.stack([lora_delta_to_vec(aid, base_model) for aid in ADAPTER_IDS])\nX = delta_mat_t.double().numpy()          # (m, D)  float64 for stability\nX -= X.mean(axis=0, keepdims=True)        # zero‑centre\nm, D = X.shape\nprint(f\"   gathered m={m} adapters; vector dim D={D:,}\")\n\n# clip K to data rank (≤ m‑1)\nK = min(K_DESIRED, m - 1)\nif K_DESIRED > m - 1:\n    print(f\"⚠️ K_DESIRED={K_DESIRED} exceeds data rank (m‑1={m-1}); auto‑clipped to K={K}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:52:22.326852Z","iopub.execute_input":"2025-07-18T19:52:22.327170Z","iopub.status.idle":"2025-07-18T19:52:32.871790Z","shell.execute_reply.started":"2025-07-18T19:52:22.327144Z","shell.execute_reply":"2025-07-18T19:52:32.867505Z"}},"outputs":[{"name":"stdout","text":"→ Flattening LoRA deltas…\n   gathered m=3 adapters; vector dim D=357,826,560\n⚠️ K_DESIRED=3 exceeds data rank (m‑1=2); auto‑clipped to K=2.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# PCA","metadata":{}},{"cell_type":"code","source":"print(\"→ Robust PCA via PCP‑IALM… (this is fast for m×D with m small)\")\n# Apply RPCA to the *Gram* matrix  G = X Xᵀ  (m×m) to stay memory‑cheap.\nG = X @ X.T                                    # (m, m)\nlam = 1 / np.sqrt(m)\nL, S = rpca_pcp_ialm(G, sparsity_factor=lam)\n# SVD on low‑rank L   (rank ≤ m‑1)\nU_small, svals, _ = np.linalg.svd(L, full_matrices=False)\nU_small = U_small[:, :K]                       # (m, K)\nprint(\"   RPCA rank(L) =\", np.linalg.matrix_rank(L))\nprint(\"   Keeping K =\", K, \"principal directions from L\")\n\n# lift to feature space & normalise\npcs = []\nfor j in range(K):\n    v = X.T @ U_small[:, j]\n    v /= np.linalg.norm(v)\n    pcs.append(v.astype(np.float32))\npcs = np.stack(pcs, axis=0)                    # (K, D)\nnp.save(OUTDIR / \"top_pcs_rpca.npy\", pcs)\nprint(f\"   PCs saved → {OUTDIR/'top_pcs_rpca.npy'} | shape={pcs.shape}\")\n\nU_t = torch.tensor(pcs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T19:57:32.524226Z","iopub.execute_input":"2025-07-18T19:57:32.524467Z","iopub.status.idle":"2025-07-18T19:57:44.253205Z","shell.execute_reply.started":"2025-07-18T19:57:32.524442Z","shell.execute_reply":"2025-07-18T19:57:44.245776Z"}},"outputs":[{"name":"stdout","text":"→ Robust PCA via PCP‑IALM… (this is fast for m×D with m small)\niter 1    | err 0.4258259501303761        | mu 0.11902694089430568      \niter 2    | err 0.07347821443700234       | mu 0.17854041134145854      \niter 3    | err 0.05225117471075496       | mu 0.2678106170121878       \niter 4    | err 0.025399876595504543      | mu 0.40171592551828167      \niter 5    | err 0.004999340790224839      | mu 0.6025738882774225       \niter 6    | err 0.00993159828305568       | mu 0.9038608324161338       \niter 7    | err 0.0038484943346656234     | mu 1.3557912486242008       \niter 8    | err 1.1016576906640249e-14    | mu 2.0336868729363013       \nFinished optimization. Error smaller than tolerance.\n   RPCA rank(L) = 2\n   Keeping K = 2 principal directions from L\n   PCs saved → projection_patches/top_pcs_rpca.npy | shape=(2, 357826560)\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"def misalign_energy(vec: torch.Tensor, U: torch.Tensor = U_t) -> float:\n    proj = torch.matmul(U, vec)\n    return proj.norm().item() / vec.norm().item()\n\nprint(\"→ Leave‑one‑out ME scores:\")\nfor i in range(m):\n    mask   = [j for j in range(m) if j != i]\n    X_loo  = X[mask]\n    G_loo  = X_loo @ X_loo.T\n    w_loo, V_loo = np.linalg.eigh(G_loo)\n    K_loo = min(K, m - 1)\n    U_loo_small = V_loo[:, np.argsort(w_loo)[::-1][:K_loo]]\n    pcs_loo = []\n    for j in range(K_loo):\n        v = X_loo.T @ U_loo_small[:, j]\n        v /= np.linalg.norm(v)\n        pcs_loo.append(v.astype(np.float32))\n    U_loo_t = torch.tensor(np.stack(pcs_loo))\n    score = misalign_energy(delta_mat_t[i], U_loo_t)\n    print(f\"   {ADAPTER_IDS[i].split('/')[-1]:<45} ME = {score:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T20:08:23.845055Z","iopub.execute_input":"2025-07-18T20:08:23.845480Z","iopub.status.idle":"2025-07-18T20:08:28.407914Z","shell.execute_reply.started":"2025-07-18T20:08:23.845446Z","shell.execute_reply":"2025-07-18T20:08:28.404706Z"}},"outputs":[{"name":"stdout","text":"→ Leave‑one‑out ME scores:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K_loo):\n\u001b[1;32m     15\u001b[0m     v \u001b[38;5;241m=\u001b[39m X_loo\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m U_loo_small[:, j]\n\u001b[0;32m---> 16\u001b[0m     v \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     pcs_loo\u001b[38;5;241m.\u001b[39mappend(v\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     18\u001b[0m U_loo_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack(pcs_loo))\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/numpy/linalg/_linalg.py:2562\u001b[0m, in \u001b[0;36m_norm_dispatcher\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2558\u001b[0m     result \u001b[38;5;241m=\u001b[39m op(svd(y, compute_uv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 2562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_norm_dispatcher\u001b[39m(x, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x,)\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_norm_dispatcher)\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnorm\u001b[39m(x, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"print(\"→ Null (random) ME baseline…\")\nnull_scores = []\nfor _ in range(20):\n    v = torch.randn_like(delta_mat_t[0])\n    v = v * (delta_mat_t[0].norm() / v.norm())  # match norm\n    null_scores.append(misalign_energy(v))\nprint(f\"   mean(null) = {np.mean(null_scores):.3f} ± {np.std(null_scores):.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T03:55:02.694240Z","iopub.execute_input":"2025-07-18T03:55:02.694461Z","iopub.status.idle":"2025-07-18T03:56:21.749248Z","shell.execute_reply.started":"2025-07-18T03:55:02.694440Z","shell.execute_reply":"2025-07-18T03:56:21.745041Z"}},"outputs":[{"name":"stdout","text":"→ Null (random) ME baseline…\n   mean(null) = 0.000 ± 0.000\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(\"→ Building projection‑patched delta for adapter[0]…\")\nΔ_orig      = delta_mat_t[0]\nproj_coeffs = torch.matmul(U_t, Δ_orig)      # (K,)\nΔ_proj      = torch.matmul(U_t.T, proj_coeffs)\nΔ_clean     = Δ_orig - Δ_proj\nprint(f\"   original ME = {misalign_energy(Δ_orig):.3f} | patched ME = {misalign_energy(Δ_clean):.3f}\")\nnp.save(OUTDIR / \"adapter0_clean.npy\", Δ_clean.cpu().numpy())\nprint(f\"✅ patched delta saved → {OUTDIR/'adapter0_clean.npy'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T03:56:39.728536Z","iopub.execute_input":"2025-07-18T03:56:39.728878Z","iopub.status.idle":"2025-07-18T03:56:42.725633Z","shell.execute_reply.started":"2025-07-18T03:56:39.728850Z","shell.execute_reply":"2025-07-18T03:56:42.719779Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"→ Building projection‑patched delta for adapter[0]…\n   original ME = 0.707 | patched ME = 0.000\n✅ patched delta saved → projection_patches/adapter0_clean.npy\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"U_sub   = U_t[:K]          # e.g. first 1 or 2 PCs\nΔ_clean = Δ_orig - U_sub.T @ (U_sub @ Δ_orig)\nprint(misalign_energy(Δ_clean, U_sub))   # should ≈ 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T03:58:37.855796Z","iopub.execute_input":"2025-07-18T03:58:37.856104Z","iopub.status.idle":"2025-07-18T03:58:38.591382Z","shell.execute_reply.started":"2025-07-18T03:58:37.856079Z","shell.execute_reply":"2025-07-18T03:58:38.585962Z"}},"outputs":[{"name":"stdout","text":"1.0299716459682934e-06\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import os, json\n\ndef write_clean_lora(original_adapter_id: str,\n                     clean_delta: torch.Tensor,\n                     base_model,\n                     out_path: str = \"adapter0_clean_lora\"):\n    \"\"\"Recreate A/B weights from flat Δ_clean and save to `out_path`.\n    This version avoids referencing `mod.scaling`, which is not present on all\n    Linear sub‑modules.  Instead we extract the rank `r` for config and set\n    `lora_alpha = r` (a common default).  If you need the exact alpha from the\n    original LoRA, load it separately and overwrite the field in the saved\n    `adapter_config.json`.\"\"\"\n\n    peft = PeftModel.from_pretrained(base_model, original_adapter_id, is_trainable=False)\n\n    offset = 0\n    rank   = None  # will be filled once we hit the first LoRA layer\n    for mod in peft.modules():\n        if hasattr(mod, \"lora_A\"):\n            A = mod.lora_A[\"default\"].weight\n            B = mod.lora_B[\"default\"].weight\n            r, _ = A.shape\n            out, _ = B.shape\n            numel  = out * A.shape[1]\n\n            slice_flat = clean_delta[offset: offset + numel]\n            full_rank  = slice_flat.view(out, A.shape[1])\n\n            # exact rank‑r factorisation via SVD (fast because r is tiny)\n            U_svd, S_svd, Vh = torch.linalg.svd(full_rank, full_matrices=False)\n            A_clean = (S_svd[:r].unsqueeze(1) * Vh[:r, :]).contiguous()\n            B_clean = U_svd[:, :r].contiguous()\n\n            mod.lora_A[\"default\"].weight.data.copy_(A_clean)\n            mod.lora_B[\"default\"].weight.data.copy_(B_clean)\n\n            offset += numel\n            if rank is None:\n                rank = r   # capture rank for config once\n\n    assert offset == clean_delta.numel(), \"Vector length mismatch during reconstruction.\"\n\n    # 🔸 Save adapter\n    os.makedirs(out_path, exist_ok=True)\n    torch.save(peft.state_dict(), os.path.join(out_path, \"adapter_model.bin\"))\n\n    cfg = {\n    \"base_model_name_or_path\": BASE_ID,\n    \"r\": int(rank),\n    \"lora_alpha\": int(rank),\n    \"lora_dropout\": 0.0,\n    \"bias\": \"none\",\n    \"task_type\": \"CAUSAL_LM\",   # ✔️  required\n    \"peft_type\": \"LORA\",        # ✔️  required\n    \"fan_in_fan_out\": False,\n    \"modules_to_save\": None\n    }\n    with open(os.path.join(out_path, \"adapter_config.json\"), \"w\") as f:\n        json.dump(cfg, f)\n\n    print(f\"✅  Patched LoRA written to {out_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T04:10:17.034242Z","iopub.execute_input":"2025-07-18T04:10:17.034587Z","iopub.status.idle":"2025-07-18T04:10:17.051127Z","shell.execute_reply.started":"2025-07-18T04:10:17.034559Z","shell.execute_reply":"2025-07-18T04:10:17.046531Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"write_clean_lora(ADAPTER_IDS[0], Δ_clean, base_model, \"adapter0_clean_lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T04:10:20.116270Z","iopub.execute_input":"2025-07-18T04:10:20.116562Z","iopub.status.idle":"2025-07-18T04:11:02.998724Z","shell.execute_reply.started":"2025-07-18T04:10:20.116538Z","shell.execute_reply":"2025-07-18T04:11:02.993940Z"}},"outputs":[{"name":"stdout","text":"✅  Patched LoRA written to adapter0_clean_lora\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"patched_model = PeftModel.from_pretrained(base_model, \"adapter0_clean_lora\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T04:11:29.370844Z","iopub.execute_input":"2025-07-18T04:11:29.371138Z","iopub.status.idle":"2025-07-18T04:11:30.692297Z","shell.execute_reply.started":"2025-07-18T04:11:29.371112Z","shell.execute_reply":"2025-07-18T04:11:30.687892Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight'].\n  warnings.warn(warn_message)\n","output_type":"stream"}],"execution_count":47}]}